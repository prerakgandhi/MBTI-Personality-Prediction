# -*- coding: utf-8 -*-
"""CS725-Models1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wIGhuy-kj5dgUGgOvSYVMpZpSHeYpzAM
"""

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
from tqdm import tqdm
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC,LinearSVC
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
from imblearn.over_sampling import SMOTE
import plotly.express as px
import warnings
warnings.filterwarnings('ignore')

from nltk.corpus import wordnet
import nltk
nltk.download('wordnet')

from google.colab import files
uploaded = files.upload()
import io
dt = pd.read_csv(io.BytesIO(uploaded['mbti_1.csv']))

data = pd.read_csv('/content/mbti_1.csv')

data.head()

data['type'].value_counts()

data['posts']

# Remove any rows with missing values.
data = data.dropna(axis=0)

train_data,test_data=train_test_split(data,test_size=0.2,random_state=42,stratify=data.type)

"""There are many links and symbols in the text data which are needed to be 
removed.
Function to clean the text data :
"""

def clear_text(data):
    data_length=[]
    lemmatizer=WordNetLemmatizer()
    cleaned_text=[]
    for sentence in tqdm(data.posts):
        sentence=sentence.lower()
        
       # removing links from text data
        sentence=re.sub('https?://[^\s<>"]+|www\.[^\s<>"]+',' ',sentence)
    
       #  removing other symbols
        sentence=re.sub('[^0-9a-z]',' ',sentence)
    
        
        data_length.append(len(sentence.split()))
        cleaned_text.append(sentence)
    return cleaned_text,data_length

train_data.posts,train_length=clear_text(train_data)
test_data.posts,test_length=clear_text(test_data)

px.pie(train_data,names='type',title='Personality type',hole=0.3)

class Lemmatizer(object):
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
    def __call__(self, sentence):
        return [self.lemmatizer.lemmatize(word) for word in sentence.split() if len(word)>2]

vectorizer=TfidfVectorizer( max_features=5000,stop_words='english',tokenizer=Lemmatizer())
vectorizer.fit(train_data.posts)

feature_names=vectorizer.get_feature_names()
wc=WordCloud(max_words=400)
wc.generate(' '.join(word for word in feature_names[500:3500] ))
plt.figure(figsize=(20,15))
plt.axis('off')
plt.imshow(wc)

train_post=vectorizer.transform(train_data.posts).toarray()
test_post=vectorizer.transform(test_data.posts).toarray()
target_encoder=LabelEncoder()
train_target=target_encoder.fit_transform(train_data.type)
test_target=target_encoder.fit_transform(test_data.type)

"""# ***Logistic Regression***

Hyper Paramter Optimization
"""

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
model = LogisticRegression()
#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
space = dict()
space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']
space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']
space['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]
search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1)
X1,X2=train_test_split(train_post,test_size=0.1,random_state=42)
Y1,Y2=train_test_split(train_target,test_size=0.1,random_state=42)
#result = search.fit(train_post,train_target)
result = search.fit(X2,Y2)
result.best_score_
result.best_params_

models_accuracy={}
model_log=LogisticRegression(penalty='l2',solver='lbfgs',max_iter=3000,C=10,n_jobs=-1)
#model_log=LogisticRegression(penalty='l1',solver='liblinear',max_iter=3000,C=10,n_jobs=-1)
model_log.fit(train_post,train_target)
print('train classification report \n ',classification_report(train_target,model_log.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))
print('test classification report \n',classification_report(test_target,model_log.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

accuracy_score(test_target,model_log.predict(test_post))

models_accuracy['logistic regression']=accuracy_score(test_target,model_log.predict(test_post))

"""# ***Linear Support Vector classifier***"""

model_linear_svc=LinearSVC(C=1)
model_linear_svc.fit(train_post,train_target)

print('train classification report \n ',classification_report(train_target,model_linear_svc.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))
print('test classification report \n',classification_report(test_target,model_linear_svc.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

models_accuracy['Linear Support Vector classifier']=accuracy_score(test_target,model_linear_svc.predict(test_post))

"""# ***Support Vector classifier***"""

#model_svc=SVC()
#model_svc.fit(train_post,train_target)

#print('train classification report \n ',classification_report(train_target,model_svc.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))
#print('test classification report \n ',classification_report(test_target,model_svc.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

"""# ***Multinomial Naive Bayes***"""

model_multinomial_nb=MultinomialNB(alpha=0.1, fit_prior=False, class_prior=None)
model_multinomial_nb.fit(train_post,train_target)

print('train classification report \n ',classification_report(train_target,model_multinomial_nb.predict(train_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))
print('test classification report \n ',classification_report(test_target,model_multinomial_nb.predict(test_post),target_names=target_encoder.inverse_transform([i for i in range(16)])))

models_accuracy['Multinomial Naive Bayes']=accuracy_score(test_target,model_multinomial_nb.predict(test_post))

accuarcy=pd.DataFrame(models_accuracy.items(),columns=['Models','Test accuracy'])
accuarcy.sort_values(by='Test accuracy',ascending=False,ignore_index=True).style.background_gradient(cmap='Blues')